# -*- coding: utf-8 -*-
"""read pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zlu9Z5Io7zpQgQWPKbcYu8xKmwfqz4Kz
"""

pip install gradio groq



# Complete RAG System with Groq LLM - Google Colab Ready
# Supports PDF, Excel, and Text files with Gradio UI

# ============================================================================
# INSTALLATION CELL - Run this first
# ============================================================================

!pip install -q groq langchain langchain-community chromadb sentence-transformers
!pip install -q gradio pypdf openpyxl pandas python-docx


# ============================================================================
# IMPORTS
# ============================================================================
import os
import tempfile
import re
from typing import List, Tuple, Optional
from pathlib import Path

import gradio as gr
from groq import Groq
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import pandas as pd
import pypdf as PyPDF2
from docx import Document as DocxDocument

# ============================================================================
# DOCUMENT PARSERS
# ============================================================================

class DocumentParser:
    """Handles parsing of multiple file formats"""

    @staticmethod
    def parse_pdf(file_path: str) -> str:
        """Extract text from PDF files"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = []
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text.append(page.extract_text())
                return "\n\n".join(text)
        except Exception as e:
            return f"Error parsing PDF: {str(e)}"

    @staticmethod
    def parse_excel(file_path: str) -> str:
        """Extract text from Excel files"""
        try:
            # Read all sheets
            excel_file = pd.ExcelFile(file_path)
            all_text = []

            for sheet_name in excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                all_text.append(f"=== Sheet: {sheet_name} ===")
                all_text.append(df.to_string(index=False))
                all_text.append("\n")

            return "\n".join(all_text)
        except Exception as e:
            return f"Error parsing Excel: {str(e)}"

    @staticmethod
    def parse_text(file_path: str) -> str:
        """Extract text from plain text files"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                return file.read()
        except Exception as e:
            return f"Error parsing text file: {str(e)}"

    @staticmethod
    def parse_docx(file_path: str) -> str:
        """Extract text from DOCX files"""
        try:
            doc = DocxDocument(file_path)
            text = []
            for paragraph in doc.paragraphs:
                text.append(paragraph.text)
            return "\n".join(text)
        except Exception as e:
            return f"Error parsing DOCX: {str(e)}"

    @classmethod
    def parse_file(cls, file_path: str) -> Tuple[str, str]:
        """
        Parse any supported file format
        Returns: (text_content, status_message)
        """
        file_extension = Path(file_path).suffix.lower()

        try:
            if file_extension == '.pdf':
                text = cls.parse_pdf(file_path)
            elif file_extension in ['.xlsx', '.xls']:
                text = cls.parse_excel(file_path)
            elif file_extension in ['.txt', '.md', '.csv']:
                text = cls.parse_text(file_path)
            elif file_extension == '.docx':
                text = cls.parse_docx(file_path)
            else:
                return "", f"Unsupported file format: {file_extension}"

            if text.startswith("Error"):
                return "", text

            return text, f"‚úì Successfully parsed {Path(file_path).name}"

        except Exception as e:
            return "", f"‚úó Error parsing {Path(file_path).name}: {str(e)}"

# ============================================================================
# TEXT CHUNKING
# ============================================================================

class TextChunker:
    """Intelligent text chunking with overlap"""

    @staticmethod
    def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """
        Split text into overlapping chunks

        Args:
            text: Input text to chunk
            chunk_size: Maximum size of each chunk in characters
            overlap: Number of overlapping characters between chunks

        Returns:
            List of text chunks
        """
        if not text or len(text.strip()) == 0:
            return []

        # Clean the text
        text = re.sub(r'\s+', ' ', text).strip()

        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size

            # If this is not the last chunk, try to break at a sentence or word boundary
            if end < text_length:
                # Look for sentence endings
                chunk = text[start:end]
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                last_break = max(last_period, last_newline)

                if last_break > chunk_size // 2:  # If found in reasonable position
                    end = start + last_break + 1
                else:
                    # Fall back to word boundary
                    last_space = chunk.rfind(' ')
                    if last_space > chunk_size // 2:
                        end = start + last_space

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            start = end - overlap if end < text_length else end

        return chunks

# ============================================================================
# RAG SYSTEM
# ============================================================================

class GroqRAGSystem:
    """Complete RAG system with Groq LLM"""

    def __init__(self):
        self.groq_client: Optional[Groq] = None
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.chroma_client = chromadb.Client(Settings(
            anonymized_telemetry=False,
            allow_reset=True
        ))
        self.collection = None
        self.document_count = 0
        self.chunk_count = 0
        self.api_key_valid = False

    def verify_api_key(self, api_key: str) -> Tuple[bool, str]:
        """
        Verify Groq API key validity

        Returns:
            (is_valid, message)
        """
        if not api_key or len(api_key.strip()) == 0:
            return False, "‚ùå API key is empty"

        try:
            # Initialize Groq client
            self.groq_client = Groq(api_key=api_key.strip())

            # Test with a simple completion
            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "user", "content": "Say 'API key verified' if you can read this."}
                ],
                max_tokens=50,
                temperature=0.1
            )

            if response.choices[0].message.content:
                self.api_key_valid = True
                return True, "‚úÖ API Key Verified Successfully! You can now upload documents."
            else:
                self.api_key_valid = False
                return False, "‚ùå API key validation failed: No response from Groq"

        except Exception as e:
            self.api_key_valid = False
            error_msg = str(e)
            if "401" in error_msg or "authentication" in error_msg.lower():
                return False, "‚ùå Invalid API Key. Please check your Groq API key."
            elif "rate_limit" in error_msg.lower():
                return False, "‚ùå Rate limit exceeded. Please try again later."
            else:
                return False, f"‚ùå Error verifying API key: {error_msg}"

    def initialize_vector_store(self):
        """Initialize or reset the vector store"""
        try:
            self.chroma_client.reset()
            self.collection = self.chroma_client.create_collection(
                name="documents",
                metadata={"hnsw:space": "cosine"}
            )
            self.document_count = 0
            self.chunk_count = 0
            return "‚úì Vector store initialized"
        except Exception as e:
            return f"‚úó Error initializing vector store: {str(e)}"

    def ingest_documents(self, files) -> str:
        """
        Ingest multiple documents into the vector store

        Args:
            files: List of file objects from Gradio

        Returns:
            Status message
        """
        if not self.api_key_valid:
            return "‚ùå Please verify your API key first before uploading documents."

        if not files:
            return "‚ö†Ô∏è No files provided"

        # Initialize vector store
        self.initialize_vector_store()

        results = []
        total_chunks = 0

        for file in files:
            # Parse document
            text, parse_status = DocumentParser.parse_file(file.name)

            if not text:
                results.append(f"‚úó {Path(file.name).name}: {parse_status}")
                continue

            # Chunk text
            chunks = TextChunker.chunk_text(text, chunk_size=1000, overlap=200)

            if not chunks:
                results.append(f"‚úó {Path(file.name).name}: No content to process")
                continue

            # Create embeddings and store
            filename = Path(file.name).name
            for i, chunk in enumerate(chunks):
                try:
                    embedding = self.embedding_model.encode(chunk).tolist()

                    self.collection.add(
                        embeddings=[embedding],
                        documents=[chunk],
                        metadatas=[{
                            "source": filename,
                            "chunk_id": i,
                            "total_chunks": len(chunks)
                        }],
                        ids=[f"{filename}_chunk_{i}"]
                    )
                    total_chunks += 1
                except Exception as e:
                    results.append(f"‚úó Error storing chunk {i} from {filename}: {str(e)}")

            results.append(f"‚úì {filename}: {len(chunks)} chunks processed")
            self.document_count += 1

        self.chunk_count = total_chunks

        summary = f"\n\nüìä Summary:\n"
        summary += f"‚Ä¢ Documents processed: {self.document_count}\n"
        summary += f"‚Ä¢ Total chunks created: {self.chunk_count}\n"
        summary += f"‚Ä¢ Vector store ready for queries"

        return "\n".join(results) + summary

    def retrieve_context(self, query: str, top_k: int = 5) -> List[dict]:
        """
        Retrieve relevant document chunks for a query

        Args:
            query: User query
            top_k: Number of chunks to retrieve

        Returns:
            List of relevant chunks with metadata
        """
        if not self.collection:
            return []

        try:
            # Create query embedding
            query_embedding = self.embedding_model.encode(query).tolist()

            # Search vector store
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )

            # Format results
            context_chunks = []
            if results['documents'] and len(results['documents'][0]) > 0:
                for doc, metadata, distance in zip(
                    results['documents'][0],
                    results['metadatas'][0],
                    results['distances'][0]
                ):
                    context_chunks.append({
                        "text": doc,
                        "source": metadata.get("source", "Unknown"),
                        "chunk_id": metadata.get("chunk_id", 0),
                        "relevance_score": 1 - distance  # Convert distance to similarity
                    })

            return context_chunks

        except Exception as e:
            print(f"Error retrieving context: {str(e)}")
            return []

    def generate_answer(self, query: str, context_chunks: List[dict]) -> Tuple[str, str]:
        """
        Generate answer using Groq LLM

        Returns:
            (answer, sources)
        """
        if not self.groq_client:
            return "‚ùå Groq client not initialized. Please verify your API key.", ""

        if not context_chunks:
            return "‚ùå No relevant information found in the uploaded documents.", ""

        # Prepare context
        context_text = "\n\n".join([
            f"[Source: {chunk['source']}, Chunk: {chunk['chunk_id']+1}]\n{chunk['text']}"
            for chunk in context_chunks
        ])

        # Create prompt
        system_prompt = """You are a helpful AI assistant that answers questions based strictly on the provided context from uploaded documents.

Rules:
1. Answer ONLY using information from the provided context
2. If the context doesn't contain enough information, say so clearly
3. Cite the source document name when providing information
4. Be concise but comprehensive
5. Do not make up or infer information not present in the context"""

        user_prompt = f"""Context from uploaded documents:
{context_text}

Question: {query}

Please provide a detailed answer based solely on the context above. Cite sources when possible."""

        try:
            # Generate response
            response = self.groq_client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.2,
                max_tokens=2000,
                top_p=0.9
            )

            answer = response.choices[0].message.content

            # Prepare sources
            sources = "\n\nüìö **Sources Used:**\n"
            unique_sources = {}
            for chunk in context_chunks:
                source = chunk['source']
                if source not in unique_sources:
                    unique_sources[source] = []
                unique_sources[source].append(chunk['chunk_id'] + 1)

            for source, chunks in unique_sources.items():
                sources += f"‚Ä¢ {source} (Chunks: {', '.join(map(str, chunks))})\n"

            return answer, sources

        except Exception as e:
            return f"‚ùå Error generating answer: {str(e)}", ""

    def query(self, question: str, top_k: int = 5) -> str:
        """
        Complete RAG query pipeline

        Args:
            question: User question
            top_k: Number of relevant chunks to retrieve

        Returns:
            Formatted answer with sources
        """
        if not self.api_key_valid:
            return "‚ùå Please verify your API key first."

        if not self.collection or self.chunk_count == 0:
            return "‚ö†Ô∏è No documents have been uploaded yet. Please upload documents first."

        if not question or len(question.strip()) == 0:
            return "‚ö†Ô∏è Please enter a question."

        # Retrieve relevant context
        context_chunks = self.retrieve_context(question, top_k)

        if not context_chunks:
            return "‚ùå No relevant information found in the uploaded documents for your query."

        # Generate answer
        answer, sources = self.generate_answer(question, context_chunks)

        # Format final response
        response = f"**Answer:**\n\n{answer}\n\n{sources}"

        return response

    def get_stats(self) -> str:
        """Get system statistics"""
        stats = f"""
üìä **System Statistics:**

‚Ä¢ API Key Status: {'‚úÖ Verified' if self.api_key_valid else '‚ùå Not Verified'}
‚Ä¢ Documents Indexed: {self.document_count}
‚Ä¢ Total Chunks: {self.chunk_count}
‚Ä¢ Vector Store: {'‚úÖ Active' if self.collection else '‚ùå Not Initialized'}
‚Ä¢ Embedding Model: all-MiniLM-L6-v2
‚Ä¢ LLM Model: Groq Llama 3.3 70B
"""
        return stats

# ============================================================================
# GRADIO INTERFACE
# ============================================================================

def create_gradio_interface():
    """Create the Gradio UI"""

    # Initialize RAG system
    rag_system = GroqRAGSystem()

    # Define interface functions
    def verify_key(api_key):
        is_valid, message = rag_system.verify_api_key(api_key)
        stats = rag_system.get_stats()
        return message, stats

    def upload_docs(files):
        result = rag_system.ingest_documents(files)
        stats = rag_system.get_stats()
        return result, stats

    def ask_question(question, top_k):
        return rag_system.query(question, top_k)

    def clear_system():
        rag_system.initialize_vector_store()
        rag_system.api_key_valid = False
        rag_system.groq_client = None
        return "System cleared. Please verify API key again.", rag_system.get_stats()

    # Create Gradio interface
    with gr.Blocks(title="RAG System with Groq", theme=gr.themes.Soft()) as demo:

        gr.Markdown("""
        # üöÄ RAG System with Groq LLM

        Upload documents (PDF, Excel, Text, DOCX) and ask questions powered by Groq's ultra-fast inference.
        """)

        with gr.Row():
            with gr.Column(scale=2):
                # API Key Section
                gr.Markdown("### üîë Step 1: API Key Verification")
                api_key_input = gr.Textbox(
                    label="Groq API Key",
                    placeholder="Enter your Groq API key (get it from console.groq.com)",
                    type="password",
                    lines=1
                )
                with gr.Row():
                    verify_btn = gr.Button("üîç Verify API Key", variant="primary")
                    clear_btn = gr.Button("üóëÔ∏è Clear System", variant="stop")

                verification_output = gr.Textbox(
                    label="Verification Status",
                    interactive=False,
                    lines=2
                )

            with gr.Column(scale=1):
                stats_output = gr.Textbox(
                    label="System Statistics",
                    value=rag_system.get_stats(),
                    interactive=False,
                    lines=10
                )

        gr.Markdown(" inexperienced")

        # Document Upload Section
        gr.Markdown("### üìÑ Step 2: Upload Documents")
        file_upload = gr.File(
            label="Upload Documents (PDF, Excel, Text, DOCX)",
            file_count="multiple",
            file_types=[".pdf", ".xlsx", ".xls", ".txt", ".md", ".csv", ".docx"]
        )
        upload_btn = gr.Button("üì§ Process Documents", variant="primary", size="lg")
        upload_output = gr.Textbox(
            label="Upload Status",
            interactive=False,
            lines=10
        )

        gr.Markdown(" inexperienced")

        # Query Section
        gr.Markdown("### üí¨ Step 3: Ask Questions")
        with gr.Row():
            with gr.Column(scale=4):
                question_input = gr.Textbox(
                    label="Your Question",
                    placeholder="Ask a question about your documents...",
                    lines=2
                )
            with gr.Column(scale=1):
                top_k_slider = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=5,
                    step=1,
                    label="Chunks to Retrieve"
                )

        query_btn = gr.Button("üîç Get Answer", variant="primary", size="lg")
        answer_output = gr.Markdown(label="Answer")

        # Event handlers
        verify_btn.click(
            fn=verify_key,
            inputs=[api_key_input],
            outputs=[verification_output, stats_output]
        )

        upload_btn.click(
            fn=upload_docs,
            inputs=[file_upload],
            outputs=[upload_output, stats_output]
        )

        query_btn.click(
            fn=ask_question,
            inputs=[question_input, top_k_slider],
            outputs=[answer_output]
        )

        clear_btn.click(
            fn=clear_system,
            inputs=[],
            outputs=[verification_output, stats_output]
        )

        # Example questions
        gr.Markdown("""
        ### üí° Tips:
        - Get your API key from [console.groq.com](https://console.groq.com)
        - Supported formats: PDF, Excel (.xlsx, .xls), Text (.txt, .md, .csv), DOCX
        - Upload multiple files at once
        - Adjust "Chunks to Retrieve" for more/less context
        - Questions are answered strictly from uploaded documents
        """)

    return demo

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Create and launch the interface
    demo = create_gradio_interface()

    # Launch with public link for Colab
    demo.launch(
        share=True,  # Creates public link
        debug=True,
        server_name="0.0.0.0",
        server_port=7860
    )